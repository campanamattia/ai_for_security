{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature estractor: manca il commento\n",
    "\n",
    "# NON E' IMPORTANTE FEATURE ESTRACTOR IN SE, E' SOLO UNO DEGLI HIDDEN LAYER DELLA NOSTRA RETE, TUTTA LA CELLA SERVE PER DEFINIRE CLASSI USATE PER LA CREAZIONE DELLA NOSTRA NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DA SOTTOLINEARE COME MAI USIAMO TRE DATASET E IN PARTICOLARE QUELLO SENZA LA SELEZIONE DI FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the aim is to confirm the results obtained using the Random Forest adopting another ensemble method. The following code processes data for binary classification with XGBoost algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choosing the number of trees to use in the ensemble\n",
    "\n",
    "In a Random Forest, each tree is trained on a subset of the training data selected by bootstrap sampling. The \"out-of-bag\" (OOB) data is the portion of the training data that was not selected during the bootstrap sampling. The OOB error is a measure of the model predictive performance.\n",
    "\n",
    "Observing the OOB estimation variation with the number of trees helps understanding after how many trees the model stabilizes, thus what's the optimal number of trees to train our RandomForest on. The point where the OOB error stops improving/fluctuating significantly indicates that the model generalizes well to unseen data, and adding more trees does not meaningfully improve the model performance.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
