{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Internet of Medical Things (IoMT) has become increasingly vital in healthcare, enabling continuous patient monitoring and automated medical services. However, connectivity capabilities also introduce cybersecurity risks that could compromise patient care and privacy. The CICIoMT2024 dataset, developed by the Canadian Institute for Cybersecurity (CIC), provides a comprehensive benchmark for evaluating IoMT security solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rate e header lenght"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RATE**: all in all, across all features Variance and IAT are strong indicators to differentiate attacks from benign traffic, especially DoS, DDoS and MQTT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BINARY CLASSIFICATION-BALANCED DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BINARY-LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we implemented a LogisticRegression model to classify attacks and benign data. Optimising regularization parameter C (inverse of regularization strength), using GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BINARY - RANDOM FOREST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we implemented an emsemble learning method for binary classification, Random Forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choosing the number of trees to use in the ensemble\n",
    "\n",
    "In a Random Forest, each tree is trained on a subset of the training data selected by bootstrap sampling. The \"out-of-bag\" (OOB) data is the portion of the training data that was not selected during the bootstrap sampling. The OOB error is a measure of the model predictive performance.\n",
    "\n",
    "Observing the OOB estimation variation with the number of trees helps understanding after how many trees the model stabilizes, thus what's the optimal number of trees to train our RandomForest on. The point where the OOB error stops improving/fluctuating significantly indicates that the model generalizes well to unseen data, and adding more trees does not meaningfully improve the model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the aim is to confirm the results obtained using the Random Forest adopting another ensemble method. The following code processes data for binary classification with XGBoost algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALIBRATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calibration plot shown below compares the probabilities predicted by the Logistic Regression and the Random Forest with the real observed frequency of positive values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to observe the opposite behavior of the two classifiers. The blue line of Logistic Regression is closer to the one representing the perfect calibration (where the predicted probability exactly matches the observed probability) for low probability values, conversely for the Random Forest represented by the orange line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC-AUC CURVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting ROC curve helps us understand sensibility and specificity of the two models, representing the ratio between true positive rate and false positives rate (false alarms). The distribution of both is excellent, but the Random Forest performs better, as suggested by the area under the curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the following sections is to evaluate the classification metrics of the two models, in order to compare them on the validation and test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VALIDATION \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTI-CLASS CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is a necessary step to prepare the dataset for a multiclass classification task, converting non-numerical labels and dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As concerns multi-class classification problems as the one considered, supposedly linear models are not able to capture the complexity of structures and relationships within data. The following code implements a Logistic Regression model to verify this assumption. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOMFOREST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non linear models are more advisable, in this section a Random Forest is implemented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to what we did in the binary classification, in this section we are going to run an XGBoost model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " DA SOTTOLINEARE COME MAI USIAMO TRE DATASET E IN PARTICOLARE QUELLO SENZA LA SELEZIONE DI FEATURES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNSUPERVISED LEARNING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements preprocessing function in order to rebalance and rescale data, which are beneficial before executing clustering algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we decided to run K-Means and DBSCAN clustering, whose results are evaluated on the basis of specified metrics, respectively the Silhouette, the Calinski-Harabasz and Davies-Bouldin scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As regards K-Means, in line with what was explained previously for NN, we decided to implement the model on both 5 and 6 classes, due to the similarities between DoS and DDos attacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While for DBSCAN the clustering task is performed adopting various eps  (representing the maximum distance between two samples to be considered in the same neighborhood) and min_samples (the minimum number of samples required to form a cluster) parameters. Once completed, the best result is saved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the visualization phase, we have choosen t-SNE for dimensionality reduction because the resulting plot is clearer and the readability enhanced with respect to other techniques, such as PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for each method are shown in a summary table reporting the selected metrics for comparison purposes. Likewise, the plots below compare side to side the clustering results obtain by the three methods. \n",
    "As observable, despite the fact that the clustering methods succeed in performing the task of separating clusters, they are not effective in identifying the classes needed for detection of malicious events. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
