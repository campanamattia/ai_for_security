{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import BayesSearchCV\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "from rich import print\n",
    "from typing import Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_multiclass(df: pd.DataFrame, column: str = 'category') -> Tuple[pd.DataFrame, pd.DataFrame, LabelEncoder]:\n",
    "    \"\"\"\n",
    "    Preprocesses the input DataFrame for a multiclass classification task.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column (str, optional): The target column. Defaults to 'category'.\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, LabelEncoder]: Training features, validation features, training labels, validation labels, and the LabelEncoder.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Encode categorical target\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df[column])\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    X = df.drop(['is_benign', 'attack', 'category'], axis=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, random_state=37)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name=\"Model\", class_names=None):\n",
    "    \"\"\"\n",
    "    Print comprehensive model evaluation metrics with both rich text output and seaborn heatmap.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    model_name : str, optional\n",
    "        Name of the model for display purposes\n",
    "    class_names : list, optional\n",
    "        List of class names for axis labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate core metrics\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, average='weighted'),\n",
    "        \"Recall\": recall_score(y_true, y_pred, average='weighted'),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    # Create metrics table\n",
    "    table_data = [[metric, f\"{value:.5f}\"] for metric, value in metrics.items()]\n",
    "    table = tabulate(table_data, headers=[\"Metric\", \"Score\"], tablefmt=\"psql\")\n",
    "    \n",
    "    print(table)\n",
    "    \n",
    "    # Calculate and plot confusion matrix as heatmap\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    if class_names is None:\n",
    "        class_names = [f\"Class {i}\" for i in range(cm.shape[0])]\n",
    "    \n",
    "    sns.heatmap(\n",
    "        cm_normalized,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names\n",
    "    )\n",
    "    plt.title(f'{model_name} - Normalized Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting our scorer to fit a multiclass classification\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Define average_score to take y_true and y_pred\n",
    "def average_score(y_true, y_pred):\n",
    "    return (accuracy_score(y_true, y_pred) + \n",
    "            precision_score(y_true, y_pred, average='macro') + \n",
    "            recall_score(y_true, y_pred, average='macro') + \n",
    "            f1_score(y_true, y_pred, average='macro')) / 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(df):\n",
    "\n",
    "    X_train, X_test, y_train, y_test, le = preprocess_data_multiclass(df)\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        num_class=len(np.unique(le.classes_)),\n",
    "        random_state=37,\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    \n",
    "    param_space = {\n",
    "        'max_depth': (5, 50),\n",
    "        'min_child_weight': (1, 6),\n",
    "        'gamma': (0, 0.5),\n",
    "        'subsample': (0.6, 1.0),\n",
    "        'colsample_bytree': (0.6, 1.0)\n",
    "    }\n",
    "\n",
    "    average_scorer = make_scorer(average_score)\n",
    "    \n",
    "    bayes_search = BayesSearchCV(\n",
    "        estimator = model,\n",
    "        search_spaces = param_space,\n",
    "        n_iter=37,\n",
    "        cv=3,\n",
    "        scoring=average_scorer,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        random_state=37\n",
    "    )\n",
    "    \n",
    "    bayes_search.fit(X_train, y_train)\n",
    "    best_model = bayes_search.best_estimator_\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    evaluate_model(y_test, y_pred, \"XGBoost Multiclass\", le.classes_)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "df = pd.read_csv('/kaggle/input/dataset/train_sel_hclust.csv')\n",
    "\n",
    "best_model = train_xgboost(df)\n",
    "best_model.save_model('xgboost_multiclass.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
