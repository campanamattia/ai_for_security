{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd30de98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T18:25:50.018516Z",
     "iopub.status.busy": "2024-12-14T18:25:50.018153Z",
     "iopub.status.idle": "2024-12-14T18:25:52.296383Z",
     "shell.execute_reply": "2024-12-14T18:25:52.295550Z"
    },
    "papermill": {
     "duration": 2.284448,
     "end_time": "2024-12-14T18:25:52.298720",
     "exception": false,
     "start_time": "2024-12-14T18:25:50.014272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from rich import print\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef70c7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T18:25:52.304740Z",
     "iopub.status.busy": "2024-12-14T18:25:52.303996Z",
     "iopub.status.idle": "2024-12-14T18:25:52.309307Z",
     "shell.execute_reply": "2024-12-14T18:25:52.308395Z"
    },
    "papermill": {
     "duration": 0.010276,
     "end_time": "2024-12-14T18:25:52.311288",
     "exception": false,
     "start_time": "2024-12-14T18:25:52.301012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data_binary(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Preprocess the input DataFrame by separating features and target, and splitting the data into training and validation sets.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing the dataset.\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.Series]: A tuple containing the training features (X_train), validation features (X_val), training target (y_train), and validation target (y_val).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(['is_benign', 'category', 'attack'], axis=1)\n",
    "    y = df['is_benign']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, random_state=37)\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aade886d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T18:25:52.316871Z",
     "iopub.status.busy": "2024-12-14T18:25:52.316529Z",
     "iopub.status.idle": "2024-12-14T18:25:52.328621Z",
     "shell.execute_reply": "2024-12-14T18:25:52.327604Z"
    },
    "papermill": {
     "duration": 0.017377,
     "end_time": "2024-12-14T18:25:52.330749",
     "exception": false,
     "start_time": "2024-12-14T18:25:52.313372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name=\"Model\", class_names=None):\n",
    "    \"\"\"\n",
    "    Print comprehensive model evaluation metrics with both rich text output and seaborn heatmap.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    model_name : str, optional\n",
    "        Name of the model for display purposes\n",
    "    class_names : list, optional\n",
    "        List of class names for axis labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate core metrics\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, average='weighted'),\n",
    "        \"Recall\": recall_score(y_true, y_pred, average='weighted'),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    # Create metrics table\n",
    "    table_data = [[metric, f\"{value:.5f}\"] for metric, value in metrics.items()]\n",
    "    table = tabulate(table_data, headers=[\"Metric\", \"Score\"], tablefmt=\"psql\")\n",
    "    \n",
    "    print(table)\n",
    "    \n",
    "    # Calculate and plot confusion matrix as heatmap\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    if class_names is None:\n",
    "        class_names = [f\"Class {i}\" for i in range(cm.shape[0])]\n",
    "    \n",
    "    sns.heatmap(\n",
    "        cm_normalized,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names\n",
    "    )\n",
    "    plt.title(f'{model_name} - Normalized Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f75a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_xgboost(df: pd.DataFrame) -> xgb.XGBClassifier:\n",
    "    \"\"\"\n",
    "    Perform grid search analysis on XGBoost classifier with improved error handling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing the dataset\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xgb.XGBClassifier\n",
    "        The best model found by grid search\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_val, y_train, y_val = preprocess_data_binary(df)\n",
    "    \n",
    "    # Initial model\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Parameter grid\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9],\n",
    "        'colsample_bytree': [0.8, 0.9]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions from best model\n",
    "    y_pred = grid_search.best_estimator_.predict(X_val)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    _ = evaluate_model(y_val, y_pred, \"XGBoost\", [\"Benign\", \"Malicious\"])\n",
    "    \n",
    "    return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c557c0d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T18:25:52.365375Z",
     "iopub.status.busy": "2024-12-14T18:25:52.365062Z",
     "iopub.status.idle": "2024-12-14T18:37:22.397451Z",
     "shell.execute_reply": "2024-12-14T18:37:22.396510Z"
    },
    "papermill": {
     "duration": 690.038349,
     "end_time": "2024-12-14T18:37:22.400309",
     "exception": false,
     "start_time": "2024-12-14T18:25:52.361960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your CSV file\n",
    "df = pd.read_csv('/kaggle/input/dataset/train_binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60368041",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model= grid_search_xgboost(df)\n",
    "best_model.save_model('xgboost_binary.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 697.545683,
   "end_time": "2024-12-14T18:37:25.024940",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-14T18:25:47.479257",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
