{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HyRSGlSJXos"
   },
   "source": [
    "# NN Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T13:56:50.735276Z",
     "iopub.status.busy": "2024-12-10T13:56:50.734411Z",
     "iopub.status.idle": "2024-12-10T13:56:50.74534Z",
     "shell.execute_reply": "2024-12-10T13:56:50.744291Z",
     "shell.execute_reply.started": "2024-12-10T13:56:50.735224Z"
    },
    "id": "cQ6F3bfaJXou",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rich import print\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T13:56:54.533793Z",
     "iopub.status.busy": "2024-12-10T13:56:54.533488Z",
     "iopub.status.idle": "2024-12-10T13:56:54.541462Z",
     "shell.execute_reply": "2024-12-10T13:56:54.540512Z",
     "shell.execute_reply.started": "2024-12-10T13:56:54.533766Z"
    },
    "id": "tukXUa8iJXov",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_loaders(df, batch_size=64, test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare data loaders for training and validation\n",
    "    \"\"\"\n",
    "    # Initialize preprocessing objects\n",
    "    label_encoder = LabelEncoder()\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Prepare features and labels\n",
    "    x_train = df.drop(['is_benign', 'category', 'attack'], axis=1).values\n",
    "    y_train = df['category']\n",
    "\n",
    "    # Encode labels as integers\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "\n",
    "    # Split data\n",
    "    x_train_data, x_val_data, y_train_data, y_val_data = train_test_split(\n",
    "        x_train, y_train, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    x_train_scaled = scaler.fit_transform(x_train_data)\n",
    "    x_val_scaled = scaler.transform(x_val_data)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "    x_val_tensor = torch.tensor(x_val_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_data, dtype=torch.long)\n",
    "    y_val_tensor = torch.tensor(y_val_data, dtype=torch.long)\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'val_dataset': val_dataset,\n",
    "        'input_size': x_train.shape[1],\n",
    "        'num_classes': len(label_encoder.classes_),\n",
    "        'class_counts': torch.bincount(y_train_tensor),\n",
    "        'y_train_tensor': y_train_tensor,\n",
    "        'classes': label_encoder.classes_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T13:56:56.641923Z",
     "iopub.status.busy": "2024-12-10T13:56:56.641243Z",
     "iopub.status.idle": "2024-12-10T13:56:56.654766Z",
     "shell.execute_reply": "2024-12-10T13:56:56.653857Z",
     "shell.execute_reply.started": "2024-12-10T13:56:56.641889Z"
    },
    "id": "UBKWQI3iJXow",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.BatchNorm1d(size),\n",
    "            nn.Linear(size, 2*size),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(2*size, size),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.block(x)\n",
    "        out += identity\n",
    "        return self.activation(out)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature extraction path\n",
    "        self.feature_extractor = FeatureExtractor(input_size)\n",
    "\n",
    "        # Main processing path with residual connections\n",
    "        self.main_path = nn.Sequential(\n",
    "          ResidualBlock(512),\n",
    "          ResidualBlock(512),\n",
    "          ResidualBlock(512),\n",
    "          nn.Linear(512, 256),\n",
    "          nn.BatchNorm1d(256),\n",
    "          nn.LeakyReLU(0.2),\n",
    "          nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.feature_extractor(x)\n",
    "\n",
    "        # Process through main path\n",
    "        main_features = self.main_path(features)\n",
    "\n",
    "        # Classification\n",
    "        output = self.classifier(main_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "def get_optimizer(model, learning_rate=0.001, weight_decay=1e-5):\n",
    "    \"\"\"\n",
    "    Create optimizer for the model\n",
    "    \"\"\"\n",
    "    return torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "def get_scheduler(optimizer):\n",
    "    \"\"\"\n",
    "    Create learning rate scheduler\n",
    "    \"\"\"\n",
    "    return torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.1,\n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "# Example of model initialization (to be used in the training loop):\n",
    "def initialize_model(input_size, num_classes, device):\n",
    "    \"\"\"\n",
    "    Initialize the model, optimizer, and scheduler\n",
    "    \"\"\"\n",
    "    model = NeuralNetwork(input_size, num_classes).to(device)\n",
    "    optimizer = get_optimizer(model)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMetrics:\n",
    "    def __init__(self):\n",
    "        self.history = {\n",
    "            'train_loss': [], \n",
    "            'val_loss': [],\n",
    "            'accuracy': [],\n",
    "        }\n",
    "    \n",
    "    def update(self, train_loss, val_loss, accuracy):\n",
    "        self.history['train_loss'].append(train_loss)\n",
    "        self.history['val_loss'].append(val_loss) \n",
    "        self.history['accuracy'].append(accuracy)\n",
    "        \n",
    "    def plot(self, dataset_name):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for metric, values in self.history.items():\n",
    "            plt.plot(values, label=metric.replace('_', ' ').title())\n",
    "        plt.title(f'Training Metrics - {dataset_name}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T13:57:06.683626Z",
     "iopub.status.busy": "2024-12-10T13:57:06.683249Z",
     "iopub.status.idle": "2024-12-10T13:57:06.701756Z",
     "shell.execute_reply": "2024-12-10T13:57:06.700993Z",
     "shell.execute_reply.started": "2024-12-10T13:57:06.683592Z"
    },
    "id": "kWa1Gc5FJXow",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_sample_weights(outputs, targets, class_counts, current_epoch, total_epochs):\n",
    "    \"\"\"Calculate sample weights based on class frequencies and curriculum stage\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Get device from outputs\n",
    "        device = outputs.device\n",
    "        \n",
    "        # Move class_counts to the same device and normalize\n",
    "        class_counts = class_counts.to(device)\n",
    "        class_freqs = class_counts / class_counts.sum()\n",
    "        max_freq = class_freqs.max()\n",
    "        class_weights = max_freq / class_freqs\n",
    "        \n",
    "        # Calculate sample difficulty based on model confidence\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        targets_idx = torch.arange(len(targets), device=device)\n",
    "        correct_probs = probs[targets_idx, targets]\n",
    "        sample_difficulties = 1 - correct_probs\n",
    "        \n",
    "        # Calculate curriculum factor (0 to 1 over epochs)\n",
    "        curriculum_factor = min(1.0, current_epoch / (total_epochs * 0.7))\n",
    "        \n",
    "        # Apply class weights based on curriculum stage\n",
    "        base_weights = class_weights[targets]\n",
    "        curriculum_weights = torch.where(\n",
    "            sample_difficulties <= curriculum_factor,\n",
    "            base_weights,\n",
    "            base_weights * 0.1  # Reduce weight for difficult samples early in training\n",
    "        )\n",
    "        \n",
    "        return curriculum_weights\n",
    "\n",
    "def train_model_with_curriculum(model, train_loader, val_loader, val_dataset, criterion, \n",
    "                             optimizer, scheduler, epochs, device, dataset_name, save_dir, goat, class_counts):\n",
    "    \n",
    "    print(f\"Training with curriculum learning on dataset: {dataset_name}\")\n",
    "    \n",
    "    metrics = TrainingMetrics()\n",
    "    best_val_loss = float('inf')\n",
    "    best_accuracy = 0.0\n",
    "    patience = 5 if goat != 0 else 10 \n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        num_batches = len(train_loader)\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        weighted_samples = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            weights = get_sample_weights(outputs, batch_y, class_counts, epoch, epochs)\n",
    "            \n",
    "            loss = criterion(outputs, batch_y)\n",
    "            weighted_loss = (loss * weights.to(device)).mean()\n",
    "            \n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += weighted_loss.item() * len(batch_y)\n",
    "            weighted_samples += weights.sum().item()\n",
    "        \n",
    "        # Normalize loss by effective number of samples\n",
    "        avg_train_loss = epoch_loss / weighted_samples\n",
    "\n",
    "\n",
    "        # Validation phase (same as train_model)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / len(val_dataset)\n",
    "        \n",
    "        metrics.update(avg_train_loss, avg_val_loss, accuracy)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, Path(save_dir) / f\"best_model_{dataset_name}.pth\")\n",
    "            \n",
    "        if best_accuracy > goat:\n",
    "            counter = 0\n",
    "            goat = best_accuracy\n",
    "            patience = 10\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping after {patience} epochs without improvement\")\n",
    "            break\n",
    "            \n",
    "    metrics.plot(dataset_name)\n",
    "    return best_accuracy, best_val_loss, goat\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, val_dataset, criterion, optimizer, \n",
    "                scheduler, epochs, device, dataset_name, save_dir, goat):\n",
    "    \n",
    "    print(f\"Training on dataset: {dataset_name}\")\n",
    "    \n",
    "    metrics = TrainingMetrics()\n",
    "    best_val_loss = float('inf')\n",
    "    best_accuracy = 0.0\n",
    "    patience = 5 if goat != 0 else 10\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = len(train_loader)\n",
    "        \n",
    "        for batch_X, batch_y in tqdm(train_loader, desc=f\"{dataset_name} - Epoch {epoch + 1}/{epochs}\"):\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        # Compute metrics\n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / len(val_dataset)\n",
    "        \n",
    "        # Update metrics history\n",
    "        metrics.update(avg_train_loss, avg_val_loss, accuracy)\n",
    "        \n",
    "        # Update best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, Path(save_dir) / f\"best_model_{dataset_name}.pth\")\n",
    "            \n",
    "        if best_accuracy > goat:\n",
    "            counter = 0\n",
    "            goat = best_accuracy\n",
    "            patience = 10\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping after {patience} epochs without improvement\")\n",
    "            break\n",
    "            \n",
    "    metrics.plot(dataset_name)\n",
    "    return best_accuracy, best_val_loss, goat\n",
    "\n",
    "def evaluate_model(model, val_loader, classes, device):\n",
    "    \"\"\"Enhanced model evaluation with custom visualizations\"\"\"\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    probs = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            probs.append(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    probs = np.concatenate(probs)\n",
    "    \n",
    "    # Plot metrics\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Custom Classification Report Visualization\n",
    "    metrics = {\n",
    "        'Precision': precision_score(y_true, y_pred, average=None),\n",
    "        'Recall': recall_score(y_true, y_pred, average=None),\n",
    "        'F1-Score': f1_score(y_true, y_pred, average=None)\n",
    "    }\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics, index=classes)\n",
    "    metrics_df.plot(kind='bar', ax=ax1)\n",
    "    ax1.set_title('Classification Metrics by Class')\n",
    "    ax1.set_xticklabels(classes, rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Enhanced Confusion Matrix\n",
    "    sns.heatmap(confusion_matrix(y_true, y_pred, normalize='true'),\n",
    "                annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes, ax=ax2)\n",
    "    ax2.set_title('Normalized Confusion Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics for logging\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'macro_f1': f1_score(y_true, y_pred, average='macro'),\n",
    "        'metrics_by_class': metrics_df\n",
    "    }\n",
    "\n",
    "def train_on_multiple_datasets(dataset_paths, save_dir='/kaggle/working', merged=False):\n",
    "    \"\"\"\n",
    "    Train the model sequentially on multiple datasets\n",
    "    \"\"\"\n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create save directory\n",
    "    save_dir = Path(save_dir)\n",
    "\n",
    "    # Training configuration\n",
    "    config = {\n",
    "        'batch_size': 64,\n",
    "        'epochs': 30,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-5,\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    goat = 0.0\n",
    "\n",
    "    # Train on each dataset sequentially\n",
    "    for dataset_path in dataset_paths:\n",
    "        dataset_name = Path(dataset_path).stem\n",
    "\n",
    "        # Load and prepare data\n",
    "        df = pd.read_csv(dataset_path, low_memory=False)\n",
    "        if merged:\n",
    "            df['category'] = df['category'].replace({\n",
    "                                        'DDoS': 'DOS_DDOS',\n",
    "                                        'DoS': 'DOS_DDOS'\n",
    "                                    })\n",
    "        \n",
    "        data = prepare_data_loaders(df, batch_size=config['batch_size'])\n",
    "        class_counts = data['class_counts'] \n",
    "\n",
    "        # Initialize model, optimizer, and criterion\n",
    "        model = NeuralNetwork(data['input_size'], data['num_classes']).to(device)\n",
    "        optimizer = get_optimizer(model, config['learning_rate'], config['weight_decay'])\n",
    "        scheduler = get_scheduler(optimizer)\n",
    "\n",
    "        # Calculate class weights for balanced training\n",
    "        total_samples = len(data['y_train_tensor'])\n",
    "        class_weights = total_samples / (len(data['class_counts']) * data['class_counts'])\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "        # Train the model\n",
    "        train_func = train_model_with_curriculum if \"train_labeled\" in dataset_name else train_model\n",
    "        \n",
    "        best_accuracy, best_val_loss, goat = train_func(\n",
    "            model=model,\n",
    "            train_loader=data['train_loader'],\n",
    "            val_loader=data['val_loader'],\n",
    "            val_dataset=data['val_dataset'],\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            epochs=config['epochs'],\n",
    "            device=device,\n",
    "            dataset_name=dataset_name,\n",
    "            save_dir=save_dir,\n",
    "            goat=goat,\n",
    "            class_counts=class_counts\n",
    "        )\n",
    "\n",
    "        # Check if the best model file exists\n",
    "        best_model_path = Path(save_dir) / f\"best_model_{dataset_name}.pth\"\n",
    "        if not best_model_path.exists():\n",
    "            print(f\"Best model file for {dataset_name} not found. Skipping evaluation.\")\n",
    "            continue\n",
    "\n",
    "        # Load best model for evaluation\n",
    "        checkpoint = torch.load(best_model_path, weights_only=True);\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # Evaluate model\n",
    "        # print(f\"\\nEvaluating model for dataset: {dataset_name}\")\n",
    "        evaluate_model(model, data['val_loader'], data['classes'], device)\n",
    "\n",
    "        # Store results\n",
    "        results[dataset_name] = {\n",
    "            'best_accuracy': best_accuracy,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "\n",
    "    # Print final results summary\n",
    "    print(\"\\nTraining Results Summary:\")\n",
    "    for dataset_name, metrics in results.items():\n",
    "        print(f\"\\n{dataset_name}:\")\n",
    "        print(f\"Best Accuracy: {metrics['best_accuracy']:.4f}\")\n",
    "        print(f\"Best Validation Loss: {metrics['best_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T13:57:11.048158Z",
     "iopub.status.busy": "2024-12-10T13:57:11.047244Z"
    },
    "id": "SStTaCSBJXox",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_paths = [\n",
    "    \"/kaggle/input/dataset/train_labeled.csv\",\n",
    "    \"/kaggle/input/dataset/train_smote.csv\",\n",
    "    \"/kaggle/input/dataset/train_sel_hclust.csv\",\n",
    "]\n",
    "\n",
    "train_on_multiple_datasets(dataset_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these result, it's clear how difficult for the model is to distinghuis between DoS and DDos. Given this fact, we'll now try to merge the two category, (of course is done with logic, at their root are attack much similar between eachother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_multiple_datasets(dataset_paths, merged = True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6273203,
     "sourceId": 10159557,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
