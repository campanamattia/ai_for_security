{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HyRSGlSJXos"
   },
   "source": [
    "# NN Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T13:56:50.735276Z",
     "iopub.status.busy": "2024-12-10T13:56:50.734411Z",
     "iopub.status.idle": "2024-12-10T13:56:50.74534Z",
     "shell.execute_reply": "2024-12-10T13:56:50.744291Z",
     "shell.execute_reply.started": "2024-12-10T13:56:50.735224Z"
    },
    "id": "cQ6F3bfaJXou",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rich import print\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T13:56:54.533793Z",
     "iopub.status.busy": "2024-12-10T13:56:54.533488Z",
     "iopub.status.idle": "2024-12-10T13:56:54.541462Z",
     "shell.execute_reply": "2024-12-10T13:56:54.540512Z",
     "shell.execute_reply.started": "2024-12-10T13:56:54.533766Z"
    },
    "id": "tukXUa8iJXov",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_loaders(df, batch_size=64, test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare data loaders for training and validation\n",
    "    \"\"\"\n",
    "    # Initialize preprocessing objects\n",
    "    label_encoder = LabelEncoder()\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Prepare features and labels\n",
    "    x_train = df.drop(['is_benign', 'category', 'attack'], axis=1).values\n",
    "    y_train = df['category']\n",
    "\n",
    "    # Encode labels as integers\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "\n",
    "    # Split data\n",
    "    x_train_data, x_val_data, y_train_data, y_val_data = train_test_split(\n",
    "        x_train, y_train, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    x_train_scaled = scaler.fit_transform(x_train_data)\n",
    "    x_val_scaled = scaler.transform(x_val_data)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "    x_val_tensor = torch.tensor(x_val_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_data, dtype=torch.long)\n",
    "    y_val_tensor = torch.tensor(y_val_data, dtype=torch.long)\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'val_dataset': val_dataset,\n",
    "        'input_size': x_train.shape[1],\n",
    "        'num_classes': len(label_encoder.classes_),\n",
    "        'class_counts': torch.bincount(y_train_tensor),\n",
    "        'y_train_tensor': y_train_tensor,\n",
    "        'classes': label_encoder.classes_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T13:56:56.641923Z",
     "iopub.status.busy": "2024-12-10T13:56:56.641243Z",
     "iopub.status.idle": "2024-12-10T13:56:56.654766Z",
     "shell.execute_reply": "2024-12-10T13:56:56.653857Z",
     "shell.execute_reply.started": "2024-12-10T13:56:56.641889Z"
    },
    "id": "UBKWQI3iJXow",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.BatchNorm1d(size),\n",
    "            nn.Linear(size, 2*size),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(2*size, size),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.block(x)\n",
    "        out += identity\n",
    "        return self.activation(out)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature extraction path\n",
    "        self.feature_extractor = FeatureExtractor(input_size)\n",
    "\n",
    "        # Main processing path with residual connections\n",
    "        self.main_path = nn.Sequential(\n",
    "          ResidualBlock(512),\n",
    "          ResidualBlock(512),\n",
    "          ResidualBlock(512),\n",
    "          nn.Linear(512, 256),\n",
    "          nn.BatchNorm1d(256),\n",
    "          nn.LeakyReLU(0.2),\n",
    "          nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.feature_extractor(x)\n",
    "\n",
    "        # Process through main path\n",
    "        main_features = self.main_path(features)\n",
    "\n",
    "        # Classification\n",
    "        output = self.classifier(main_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "def get_optimizer(model, learning_rate=0.001, weight_decay=1e-5):\n",
    "    \"\"\"\n",
    "    Create optimizer for the model\n",
    "    \"\"\"\n",
    "    return torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "def get_scheduler(optimizer):\n",
    "    \"\"\"\n",
    "    Create learning rate scheduler\n",
    "    \"\"\"\n",
    "    return torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.1,\n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "# Example of model initialization (to be used in the training loop):\n",
    "def initialize_model(input_size, num_classes, device):\n",
    "    \"\"\"\n",
    "    Initialize the model, optimizer, and scheduler\n",
    "    \"\"\"\n",
    "    model = NeuralNetwork(input_size, num_classes).to(device)\n",
    "    optimizer = get_optimizer(model)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMetrics:\n",
    "    def __init__(self):\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'accuracy': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "\n",
    "    def update(self, train_loss, val_loss, accuracy, lr):\n",
    "        self.history['train_loss'].append(train_loss)\n",
    "        self.history['val_loss'].append(val_loss)\n",
    "        self.history['accuracy'].append(accuracy)\n",
    "        self.history['learning_rates'].append(lr)\n",
    "\n",
    "    def plot(self, dataset_name):\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Plot metrics on primary y-axis\n",
    "        metrics = ['train_loss', 'val_loss', 'accuracy']\n",
    "        colors = ['blue', 'orange', 'green']\n",
    "        for metric, color in zip(metrics, colors):\n",
    "            ax1.plot(self.history[metric], \n",
    "                    label=metric.replace('_', ' ').title(),\n",
    "                    color=color)\n",
    "        \n",
    "        # Create secondary y-axis for learning rate\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_yscale('log')  # Set logarithmic scale for learning rate\n",
    "        ax2.plot(self.history['learning_rates'], \n",
    "                label='Learning Rate',\n",
    "                color='red',\n",
    "                linestyle='--')\n",
    "        \n",
    "        # Customize plot\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Metrics Value')\n",
    "        ax2.set_ylabel('Learning Rate (log scale)')\n",
    "        \n",
    "        # Combine legends from both axes\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "        \n",
    "        plt.title(f'Training Metrics - {dataset_name}')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T13:57:06.683626Z",
     "iopub.status.busy": "2024-12-10T13:57:06.683249Z",
     "iopub.status.idle": "2024-12-10T13:57:06.701756Z",
     "shell.execute_reply": "2024-12-10T13:57:06.700993Z",
     "shell.execute_reply.started": "2024-12-10T13:57:06.683592Z"
    },
    "id": "kWa1Gc5FJXow",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, val_dataset, criterion, optimizer, \n",
    "                scheduler, epochs, device, dataset_name, save_dir, goat):\n",
    "    \n",
    "    print(f\"Training on dataset: {dataset_name}\")\n",
    "    \n",
    "    metrics = TrainingMetrics()\n",
    "    best_val_loss = float('inf')\n",
    "    best_accuracy = 0.0\n",
    "    patience = 9 if goat != 0 else 15\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = len(train_loader)\n",
    "        \n",
    "        for batch_X, batch_y in tqdm(train_loader, desc=f\"{dataset_name} - Epoch {epoch + 1}/{epochs}\", bar_format='{desc}: {elapsed}'):\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        # Compute metrics\n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / len(val_dataset)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        metrics.update(avg_train_loss, avg_val_loss, accuracy, current_lr)\n",
    "        \n",
    "        # Update best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, Path(save_dir) / f\"best_model_{dataset_name}.pth\")\n",
    "            \n",
    "        if best_accuracy > goat:\n",
    "            counter = 0\n",
    "            goat = best_accuracy\n",
    "            patience = 15\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping after {patience} epochs without improvement\")\n",
    "            break\n",
    "            \n",
    "    metrics.plot(dataset_name)\n",
    "    return best_accuracy, best_val_loss, goat\n",
    "\n",
    "def evaluate_model(model, val_loader, classes, device):\n",
    "    \"\"\"Enhanced model evaluation with custom visualizations\"\"\"\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    probs = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            probs.append(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    probs = np.concatenate(probs)\n",
    "    \n",
    "    # Plot metrics\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Custom Classification Report Visualization\n",
    "    metrics = {\n",
    "        'Precision': precision_score(y_true, y_pred, average=None),\n",
    "        'Recall': recall_score(y_true, y_pred, average=None),\n",
    "        'F1-Score': f1_score(y_true, y_pred, average=None)\n",
    "    }\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics, index=classes)\n",
    "    metrics_df.plot(kind='bar', ax=ax1)\n",
    "    ax1.set_title('Classification Metrics by Class')\n",
    "    ax1.set_xticklabels(classes, rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Enhanced Confusion Matrix\n",
    "    sns.heatmap(confusion_matrix(y_true, y_pred, normalize='true'),\n",
    "                annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes, ax=ax2)\n",
    "    ax2.set_title('Normalized Confusion Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics for logging\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'macro_f1': f1_score(y_true, y_pred, average='macro'),\n",
    "        'metrics_by_class': metrics_df\n",
    "    }\n",
    "\n",
    "def train_on_multiple_datasets(dataset_paths, save_dir='/kaggle/working', merged=False):\n",
    "    \"\"\"\n",
    "    Train the model sequentially on multiple datasets\n",
    "    \"\"\"\n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create save directory\n",
    "    save_dir = Path(save_dir)\n",
    "\n",
    "    # Training configuration\n",
    "    config = {\n",
    "        'batch_size': 64,\n",
    "        'epochs': 30,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-5,\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    goat = 0.0\n",
    "\n",
    "    # Train on each dataset sequentially\n",
    "    for dataset_path in dataset_paths:\n",
    "        dataset_name = Path(dataset_path).stem\n",
    "\n",
    "        # Load and prepare data\n",
    "        df = pd.read_csv(dataset_path, low_memory=False)\n",
    "        \n",
    "        if merged:\n",
    "            df['category'] = df['category'].replace({\n",
    "                                        'DDoS': 'DOS_DDOS',\n",
    "                                        'DoS': 'DOS_DDOS'\n",
    "                                    })\n",
    "        \n",
    "        data = prepare_data_loaders(df, batch_size=config['batch_size'])\n",
    "        class_counts = data['class_counts'] \n",
    "\n",
    "        # Initialize model, optimizer, and criterion\n",
    "        model = NeuralNetwork(data['input_size'], data['num_classes']).to(device)\n",
    "        optimizer = get_optimizer(model, config['learning_rate'], config['weight_decay'])\n",
    "        scheduler = get_scheduler(optimizer)\n",
    "\n",
    "        # Calculate class weights for balanced training\n",
    "        total_samples = len(data['y_train_tensor'])\n",
    "        class_weights = total_samples / (len(data['class_counts']) * data['class_counts'])\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "        best_accuracy, best_val_loss, goat = train_model(\n",
    "            model=model,\n",
    "            train_loader=data['train_loader'],\n",
    "            val_loader=data['val_loader'],\n",
    "            val_dataset=data['val_dataset'],\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            epochs=config['epochs'],\n",
    "            device=device,\n",
    "            dataset_name=dataset_name,\n",
    "            save_dir=save_dir,\n",
    "            goat=goat\n",
    "        )\n",
    "\n",
    "        # Check if the best model file exists\n",
    "        best_model_path = Path(save_dir) / f\"best_model_{dataset_name}.pth\"\n",
    "        if not best_model_path.exists():\n",
    "            print(f\"Best model file for {dataset_name} not found. Skipping evaluation.\")\n",
    "            continue\n",
    "\n",
    "        # Load best model for evaluation\n",
    "        checkpoint = torch.load(best_model_path, weights_only=True);\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # Evaluate model\n",
    "        # print(f\"\\nEvaluating model for dataset: {dataset_name}\")\n",
    "        evaluate_model(model, data['val_loader'], data['classes'], device)\n",
    "\n",
    "        # Store results\n",
    "        results[dataset_name] = {\n",
    "            'best_accuracy': best_accuracy,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "\n",
    "    # Print final results summary in a table using tabulate\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df.columns = ['Best Accuracy', 'Best Validation Loss']\n",
    "    print(\"\\nTraining Results Summary:\")\n",
    "    print(tabulate(results_df, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T13:57:11.048158Z",
     "iopub.status.busy": "2024-12-10T13:57:11.047244Z"
    },
    "id": "SStTaCSBJXox",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_paths = [\n",
    "    \"/kaggle/input/dataset/train_labeled.csv\",\n",
    "    \"/kaggle/input/dataset/train_smote.csv\",\n",
    "    \"/kaggle/input/dataset/train_sel_hclust.csv\",\n",
    "]\n",
    "\n",
    "train_on_multiple_datasets(dataset_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these result, it's clear how difficult for the model is to distinghuis between DoS and DDos. Given this fact, we'll now try to merge the two category, (of course is done with logic, at their root are attack much similar between eachother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_multiple_datasets(dataset_paths, merged = True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6273203,
     "sourceId": 10159557,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
