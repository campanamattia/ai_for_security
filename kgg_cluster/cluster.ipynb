{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19e1d1d",
   "metadata": {
    "papermill": {
     "duration": 0.002677,
     "end_time": "2024-12-12T15:49:20.914618",
     "exception": false,
     "start_time": "2024-12-12T15:49:20.911941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CLUSTER 6 CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea30827c",
   "metadata": {},
   "source": [
    "This notebook analyzes network traffic data using clustering techniques to identify patterns in network attacks. We'll explore two different approaches:\n",
    "1. Standard K-means clustering with 6 clusters\n",
    "2. Modified approach using DBSCAN with merged DoS/DDoS categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d827f2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T15:49:20.921122Z",
     "iopub.status.busy": "2024-12-12T15:49:20.920724Z",
     "iopub.status.idle": "2024-12-12T15:49:23.660503Z",
     "shell.execute_reply": "2024-12-12T15:49:23.659444Z"
    },
    "papermill": {
     "duration": 2.745925,
     "end_time": "2024-12-12T15:49:23.663002",
     "exception": false,
     "start_time": "2024-12-12T15:49:20.917077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ee1b0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T15:49:23.669390Z",
     "iopub.status.busy": "2024-12-12T15:49:23.668548Z",
     "iopub.status.idle": "2024-12-12T15:49:30.637782Z",
     "shell.execute_reply": "2024-12-12T15:49:30.636517Z"
    },
    "papermill": {
     "duration": 6.975208,
     "end_time": "2024-12-12T15:49:30.640504",
     "exception": false,
     "start_time": "2024-12-12T15:49:23.665296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/dataset/train_sel_hclust.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4fd9f7",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df, method='median_multiplier', multiplier=2):\n",
    "    \"\"\"\n",
    "    Balance the dataset by downsampling majority classes.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with 'category' column\n",
    "    - method: str, approach for calculating threshold ('median_multiplier', 'mean_multiplier', 'quantile')\n",
    "    - multiplier: float, multiplier for threshold calculation\n",
    "    \n",
    "    Returns:\n",
    "    - balanced DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"Original dataset size: {len(df)}\")\n",
    "    print(\"Original category distribution:\")\n",
    "    print(df['category'].value_counts())\n",
    "    \n",
    "    categories = df['category'].value_counts()\n",
    "    \n",
    "    # Calculate threshold based on method\n",
    "    if method == 'median_multiplier':\n",
    "        threshold = np.median(categories) * multiplier\n",
    "    elif method == 'mean_multiplier':\n",
    "        threshold = np.mean(categories) * multiplier\n",
    "    elif method == 'quantile':\n",
    "        threshold = categories.quantile(0.75)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'median_multiplier', 'mean_multiplier', or 'quantile'\")\n",
    "    \n",
    "    # Balance each category\n",
    "    balanced_dfs = []\n",
    "    for cat in categories.index:\n",
    "        cat_df = df[df['category'] == cat]\n",
    "        if len(cat_df) > threshold:\n",
    "            cat_df = resample(cat_df, replace=False, n_samples=int(threshold), random_state=42)\n",
    "        balanced_dfs.append(cat_df)\n",
    "    \n",
    "    balanced_df = pd.concat(balanced_dfs)\n",
    "    \n",
    "    print(f\"\\nBalanced dataset size: {len(balanced_df)}\")\n",
    "    print(\"Balanced category distribution:\")\n",
    "    print(balanced_df['category'].value_counts())\n",
    "    \n",
    "    return balanced_df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Prepare features for clustering by dropping non-feature columns.\n",
    "    \"\"\"\n",
    "    return df.drop(['category', 'attack', 'is_benign'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5292e",
   "metadata": {},
   "source": [
    "## 3. Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clustering_metrics(X_scaled, clusters):\n",
    "    \"\"\"Calculate and print various clustering quality metrics.\"\"\"\n",
    "    if len(set(clusters)) > 1:\n",
    "        metrics = {\n",
    "            'Silhouette Score': silhouette_score(X_scaled, clusters),\n",
    "            'Calinski-Harabasz Score': calinski_harabasz_score(X_scaled, clusters),\n",
    "            'Davies-Bouldin Score': davies_bouldin_score(X_scaled, clusters)\n",
    "        }\n",
    "        \n",
    "        print(\"\\nClustering Quality Metrics:\")\n",
    "        for metric_name, score in metrics.items():\n",
    "            print(f\"{metric_name}: {score:.3f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def analyze_cluster_distribution(df, clusters):\n",
    "    \"\"\"Analyze and print cluster size and category distribution.\"\"\"\n",
    "    print(\"\\nCluster Size Distribution:\")\n",
    "    print(pd.Series(clusters).value_counts())\n",
    "    \n",
    "    print(\"\\nCategory Distribution within Clusters:\")\n",
    "    print(pd.crosstab(clusters, df['category']))\n",
    "\n",
    "def visualize_clusters(X_scaled, clusters, model=None, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Visualize clusters using PCA.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_scaled: scaled feature matrix\n",
    "    - clusters: cluster assignments\n",
    "    - model: fitted clustering model (optional, for showing centroids)\n",
    "    - title_suffix: additional text for plot title\n",
    "    \"\"\"\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "                         c=clusters,\n",
    "                         cmap='viridis',\n",
    "                         alpha=0.6)\n",
    "    \n",
    "    # Add cluster centers for KMeans\n",
    "    if hasattr(model, 'cluster_centers_'):\n",
    "        centers_pca = pca.transform(model.cluster_centers_)\n",
    "        plt.scatter(centers_pca[:, 0], centers_pca[:, 1],\n",
    "                   c='red', marker='x', s=200, linewidths=3,\n",
    "                   label='Cluster Centers')\n",
    "        plt.legend()\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.title(f'2D Cluster Visualization using PCA {title_suffix}')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    \n",
    "    # Print variance explained\n",
    "    total_var = sum(pca.explained_variance_ratio_)\n",
    "    print(f\"\\nTotal explained variance by two components: {total_var:.2%}\")\n",
    "    print(\"\\nIndividual explained variance ratios:\")\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "        print(f\"PC{i+1}: {ratio:.2%}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aef324",
   "metadata": {},
   "source": [
    "## 4. Analysis: K-means Clustering\n",
    "\n",
    "### Why K-means?\n",
    "\n",
    "We start with K-means clustering because:\n",
    "1. It's a simple and effective algorithm for our initial analysis\n",
    "2. We have a reasonable assumption about the number of clusters (based on attack categories)\n",
    "3. It works well with standardized numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1dbcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset\n",
    "balanced_df = balance_dataset(df)\n",
    "\n",
    "# Prepare and scale features\n",
    "X = prepare_features(balanced_df)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform K-means clustering\n",
    "start_time = time.time()\n",
    "kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Calculate metrics and visualize\n",
    "metrics = calculate_clustering_metrics(X_scaled, clusters)\n",
    "analyze_cluster_distribution(balanced_df, clusters)\n",
    "visualize_clusters(X_scaled, clusters, kmeans)\n",
    "\n",
    "print(f\"\\nClustering time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2d3b4",
   "metadata": {},
   "source": [
    "## 5. Modified Approach: DBSCAN with Merged Categories\n",
    "\n",
    "### Why modify the approach?\n",
    "\n",
    "Based on the K-means results, we observed:\n",
    "1. Significant overlap between DDoS and DoS categories\n",
    "2. Some clusters might not be spherical (a limitation of K-means)\n",
    "3. StandardScaler might not be optimal for our feature distributions\n",
    "\n",
    "Therefore, we implement these changes:\n",
    "1. Merge DDoS and DoS into a single category\n",
    "2. Use DBSCAN for non-spherical clusters\n",
    "3. Switch to MinMaxScaler for better feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74635a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify categories\n",
    "df_modified = df.copy()\n",
    "df_modified['category'] = df_modified['category'].replace({\n",
    "    'DDoS': 'DOS_DDOS',\n",
    "    'DoS': 'DOS_DDOS'\n",
    "})\n",
    "\n",
    "# Balance the modified dataset\n",
    "balanced_df_modified = balance_dataset(df_modified)\n",
    "\n",
    "# Prepare and scale features\n",
    "X_modified = prepare_features(balanced_df_modified)\n",
    "scaler_modified = MinMaxScaler()\n",
    "X_scaled_modified = scaler_modified.fit_transform(X_modified)\n",
    "\n",
    "# Perform DBSCAN clustering\n",
    "start_time = time.time()\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=10)\n",
    "clusters_modified = dbscan.fit_predict(X_scaled_modified)\n",
    "\n",
    "# Calculate metrics and visualize\n",
    "metrics_modified = calculate_clustering_metrics(X_scaled_modified, clusters_modified)\n",
    "analyze_cluster_distribution(balanced_df_modified, clusters_modified)\n",
    "visualize_clusters(X_scaled_modified, clusters_modified, title_suffix=\"(Modified Approach)\")\n",
    "\n",
    "print(f\"\\nClustering time: {time.time() - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10416.657438,
   "end_time": "2024-12-12T18:42:54.811900",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-12T15:49:18.154462",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
