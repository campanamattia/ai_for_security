{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTER 6 CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans  # Changed from MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/dataset/train_sel_hclust.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_clustering(df, method='median_multiplier', multiplier=1):\n",
    "    # Print original data size\n",
    "    print(f\"Original dataset size: {len(df)}\")\n",
    "    print(\"Original category distribution:\")\n",
    "    print(df['category'].value_counts())\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    categories = df['category'].value_counts()\n",
    "    \n",
    "    if method == 'median_multiplier':\n",
    "        threshold = np.median(categories) * multiplier\n",
    "    elif method == 'mean_multiplier':\n",
    "        threshold = np.mean(categories) * multiplier\n",
    "    elif method == 'quantile':\n",
    "        threshold = categories.quantile(0.75)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'median_multiplier', 'mean_multiplier', or 'quantile'\")\n",
    "    \n",
    "    balanced_dfs = []\n",
    "    for cat in categories.index:\n",
    "        cat_df = df[df['category'] == cat]\n",
    "        if len(cat_df) > threshold:\n",
    "            cat_df = resample(cat_df, replace=False, n_samples=int(threshold), random_state=42)\n",
    "        balanced_dfs.append(cat_df)\n",
    "    \n",
    "    balanced_df = pd.concat(balanced_dfs)\n",
    "    \n",
    "    # Print balanced data size\n",
    "    print(f\"\\nBalanced dataset size: {len(balanced_df)}\")\n",
    "    print(\"Balanced category distribution:\")\n",
    "    print(balanced_df['category'].value_counts())\n",
    "    \n",
    "    X = balanced_df.drop(['category', 'attack', 'is_benign'], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Using regular KMeans instead of MiniBatchKMeans\n",
    "    kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    balanced_df['cluster'] = clusters\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nClustering time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return balanced_df, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.utils import resample\n",
    "import time\n",
    "\n",
    "def evaluate_clustering(df, model='median_multiplier', multiplier=1):\n",
    "    print(f\"Original dataset size: {len(df)}\")\n",
    "    print(\"Original category distribution:\")\n",
    "    print(df['category'].value_counts())\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    categories = df['category'].value_counts()\n",
    "    \n",
    "    if model == 'median_multiplier':\n",
    "        threshold = np.median(categories) * multiplier\n",
    "    elif model == 'mean_multiplier':\n",
    "        threshold = np.mean(categories) * multiplier\n",
    "    elif model == 'quantile':\n",
    "        threshold = categories.quantile(0.75)\n",
    "    \n",
    "    balanced_dfs = []\n",
    "    for cat in categories.index:\n",
    "        cat_df = df[df['category'] == cat]\n",
    "        if len(cat_df) > threshold:\n",
    "            cat_df = resample(cat_df, replace=False, n_samples=int(threshold), random_state=42)\n",
    "        balanced_dfs.append(cat_df)\n",
    "    \n",
    "    balanced_df = pd.concat(balanced_dfs)\n",
    "    \n",
    "    print(f\"\\nBalanced dataset size: {len(balanced_df)}\")\n",
    "    print(\"Balanced category distribution:\")\n",
    "    print(balanced_df['category'].value_counts())\n",
    "    \n",
    "    X = balanced_df.drop(['category', 'attack', 'is_benign'], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=6, random_state=42, n_init=20)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    balanced_df['cluster'] = clusters\n",
    "    \n",
    "    # Calculate clustering quality metrics\n",
    "    silhouette = silhouette_score(X_scaled, clusters)\n",
    "    calinski = calinski_harabasz_score(X_scaled, clusters)\n",
    "    davies = davies_bouldin_score(X_scaled, clusters)\n",
    "    \n",
    "    print(\"\\nClustering Quality Metrics:\")\n",
    "    print(f\"Silhouette Score: {silhouette:.3f} (ranges from -1 to 1, higher is better)\")\n",
    "    print(f\"Calinski-Harabasz Score: {calinski:.3f} (higher is better)\")\n",
    "    print(f\"Davies-Bouldin Score: {davies:.3f} (lower is better)\")\n",
    "    \n",
    "    # Analyze cluster sizes\n",
    "    print(\"\\nCluster Size Distribution:\")\n",
    "    print(pd.Series(clusters).value_counts())\n",
    "    \n",
    "    # Analyze category distribution within clusters\n",
    "    print(\"\\nCategory Distribution within Clusters:\")\n",
    "    print(pd.crosstab(balanced_df['cluster'], balanced_df['category']))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nClustering time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return balanced_df, kmeans\n",
    "\n",
    "# Run evaluation\n",
    "balanced_df, model = evaluate_clustering(df, model='median_multiplier', multiplier=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters_2d(balanced_df, model, features):\n",
    "    # Get the feature columns\n",
    "    X = balanced_df[features]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create 2D plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot points colored by cluster\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "                          c=balanced_df['cluster'], cmap='viridis',\n",
    "                          alpha=0.6)\n",
    "    \n",
    "    # Plot cluster centers\n",
    "    if hasattr(model, 'cluster_centers_'):\n",
    "        centers_pca = pca.transform(model.cluster_centers_)\n",
    "        plt.scatter(centers_pca[:, 0], centers_pca[:, 1],\n",
    "                    c='red', marker='x', s=200, linewidths=3,\n",
    "                    label='Cluster Centers')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.title('2D Cluster Visualization using PCA')\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Print explained variance\n",
    "    total_var = sum(pca.explained_variance_ratio_)\n",
    "    print(f\"\\nTotal explained variance by two components: {total_var:.2%}\")\n",
    "    print(\"\\nIndividual explained variance ratios:\")\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "        print(f\"PC{i+1}: {ratio:.2%}\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = balanced_df.drop(['category', 'attack', 'is_benign', 'cluster'], axis=1).columns\n",
    "visualize_clusters_2d(balanced_df, model, feature_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
