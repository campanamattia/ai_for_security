{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19e1d1d",
   "metadata": {
    "papermill": {
     "duration": 0.002677,
     "end_time": "2024-12-12T15:49:20.914618",
     "exception": false,
     "start_time": "2024-12-12T15:49:20.911941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CLUSTER 6 CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea30827c",
   "metadata": {},
   "source": [
    "This notebook analyzes network traffic data using clustering techniques to identify patterns in network attacks. We'll explore two different approaches:\n",
    "1. Standard K-means clustering with 6 clusters\n",
    "2. Modified approach using DBSCAN with merged DoS/DDoS categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d827f2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T15:49:20.921122Z",
     "iopub.status.busy": "2024-12-12T15:49:20.920724Z",
     "iopub.status.idle": "2024-12-12T15:49:23.660503Z",
     "shell.execute_reply": "2024-12-12T15:49:23.659444Z"
    },
    "papermill": {
     "duration": 2.745925,
     "end_time": "2024-12-12T15:49:23.663002",
     "exception": false,
     "start_time": "2024-12-12T15:49:20.917077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "from sklearn.utils import resample\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Clustering Models or Labels\n",
    "def save_cluster(model, filename):\n",
    "    \"\"\"Save a clustering model to a file.\"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "def save_labels(labels, filename):\n",
    "    \"\"\"Save clustering labels to a file.\"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(labels, file)\n",
    "\n",
    "def load_cluster(filename):\n",
    "    \"\"\"Load a clustering model from a file.\"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    return model\n",
    "\n",
    "def load_labels(filename):\n",
    "    \"\"\"Load clustering labels from a file.\"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        labels = pickle.load(file)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4fd9f7",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f86d6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df, method='median_multiplier', multiplier=2, column='category'):\n",
    "    \"\"\"\n",
    "    Balance the dataset by downsampling majority classes.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with 'category' or 'attack' column\n",
    "    - method: str, approach for calculating threshold\n",
    "    - multiplier: float, multiplier for threshold calculation\n",
    "    \n",
    "    Returns:\n",
    "    - balanced DataFrame\n",
    "    \"\"\"\n",
    "    counts = df[column].value_counts()\n",
    "\n",
    "    # Calculate threshold\n",
    "    if method == 'median_multiplier':\n",
    "        threshold = np.median(counts) * multiplier\n",
    "    elif method == 'mean_multiplier':\n",
    "        threshold = np.mean(counts) * multiplier\n",
    "    elif method == 'quantile':\n",
    "        threshold = counts.quantile(0.75)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'median_multiplier', 'mean_multiplier', or 'quantile'\")\n",
    "    \n",
    "    # Balance categories\n",
    "    balanced_dfs = []\n",
    "    for value in counts.index:\n",
    "        value_df = df[df[column] == value]\n",
    "        if len(value_df) > threshold:\n",
    "            value_df = resample(value_df, replace=False, n_samples=int(threshold), random_state=42)\n",
    "        balanced_dfs.append(value_df)\n",
    "    \n",
    "    balanced_df = pd.concat(balanced_dfs)\n",
    "    comparison_data = [\n",
    "        [value, counts[value], balanced_df[column].value_counts().get(value, 0)]\n",
    "        for value in sorted(counts.index)\n",
    "    ]\n",
    "\n",
    "    comparison_data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(tabulate(\n",
    "        comparison_data,\n",
    "        headers=[column, 'Original', 'After Balance'],\n",
    "        tablefmt='psql'\n",
    "    ))\n",
    "    \n",
    "    return balanced_df.sample(frac=0.01, random_state=37)\n",
    "\n",
    "def preprocess_data_cluster(df, **kwargs):\n",
    "    \"\"\"Scale and balance data.\"\"\"\n",
    "\n",
    "    method = kwargs.get('method', 'median_multiplier')\n",
    "    multiplier = kwargs.get('multiplier', 2)\n",
    "    column = kwargs.get('column', 'category')\n",
    "\n",
    "    df = balance_dataset(df, method=method, multiplier=multiplier, column=column)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_df = scaler.fit_transform(df.drop(['is_benign', 'category', 'attack'], axis=1))\n",
    "\n",
    "\n",
    "    return scaled_df, df[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5292e",
   "metadata": {},
   "source": [
    "## 3. Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074512c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(data, n_clusters):\n",
    "    \"\"\"Perform K-Means clustering and evaluate results.\"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=37) \n",
    "    labels = kmeans.fit_predict(data)\n",
    "    print(f\"K-Means Clustering with {n_clusters} clusters completed.\")\n",
    "    return labels, kmeans\n",
    "\n",
    "# DBSCAN Clustering\n",
    "def dbscan_clustering(data, eps_values = [0.1, 0.3, 0.5, 0.7, 0.9], min_samples_values = [5, 10, 15, 20]):\n",
    "    \"\"\"Perform DBSCAN clustering with different parameters and save the best result.\"\"\"\n",
    "    best_score = -1\n",
    "    best_labels = None\n",
    "    best_eps = None\n",
    "    best_min_samples = None\n",
    "    \n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(data)\n",
    "            score = silhouette_score(data, labels)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_labels = labels\n",
    "                best_eps = eps\n",
    "                best_min_samples = min_samples\n",
    "    \n",
    "    print(f\"Best DBSCAN Clustering with eps={best_eps}, min_samples={best_min_samples} completed.\")\n",
    "    return best_labels\n",
    "\n",
    "# Hierarchical Clustering\n",
    "def hierarchical_clustering(data, n_clusters):\n",
    "    \"\"\"Perform Agglomerative Clustering.\"\"\"\n",
    "    hc = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    labels = hc.fit_predict(data)\n",
    "    print(f\"Hierarchical Clustering with {n_clusters} clusters completed.\")\n",
    "    return labels\n",
    "\n",
    "# Evaluate Clustering\n",
    "def evaluate_clustering(data, labels):\n",
    "    \"\"\"Evaluate clustering using different metrics.\"\"\"\n",
    "    silhouette = silhouette_score(data, labels)\n",
    "    calinski = calinski_harabasz_score(data, labels)\n",
    "    davies = davies_bouldin_score(data, labels)\n",
    "    \n",
    "    return silhouette, calinski, davies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16dffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(data, cluster_labels_list, true_labels, method_names):\n",
    "    \"\"\"\n",
    "    Visualize clustering results alongside true labels using t-SNE.\n",
    "    Enhanced for better readability and comparison.\n",
    "    \"\"\"\n",
    "    tsne = TSNE(n_components=2, random_state=37)\n",
    "    reduced_data = tsne.fit_transform(data)\n",
    "    \n",
    "    n_methods = len(cluster_labels_list)\n",
    "    fig, axes = plt.subplots(n_methods, 2, figsize=(20, 8*n_methods))\n",
    "    \n",
    "    # If only one method, wrap axes in 2D array for consistent indexing\n",
    "    if n_methods == 1:\n",
    "        axes = axes.reshape(1, 2)\n",
    "    \n",
    "    # Custom color palettes\n",
    "    pred_palette = sns.color_palette(\"Blues\", n_colors=len(cluster_labels_list))\n",
    "    true_palette = sns.color_palette(\"Reds\", n_colors=len(true_labels))\n",
    "    \n",
    "    for idx, (method_labels, method_name) in enumerate(zip(cluster_labels_list, method_names)):\n",
    "        # Plot clustering results\n",
    "        cluster_scatter = sns.scatterplot(\n",
    "            x=reduced_data[:, 0],\n",
    "            y=reduced_data[:, 1],\n",
    "            hue=method_labels,\n",
    "            palette=pred_palette,\n",
    "            alpha=0.7,\n",
    "            s=100,  # Larger point size\n",
    "            ax=axes[idx, 0]\n",
    "        )\n",
    "        axes[idx, 0].set_title(f'{method_name}\\nClustering Results', fontsize=12, pad=15)\n",
    "        cluster_scatter.legend(title=\"Clusters\", bbox_to_anchor=(1.02, 1), loc='upper left', title_fontsize=10)\n",
    "        \n",
    "        # Plot true labels\n",
    "        true_scatter = sns.scatterplot(\n",
    "            x=reduced_data[:, 0],\n",
    "            y=reduced_data[:, 1],\n",
    "            hue=true_labels,\n",
    "            palette=true_palette,\n",
    "            alpha=0.7,\n",
    "            s=100,  # Larger point size\n",
    "            ax=axes[idx, 1]\n",
    "        )\n",
    "        axes[idx, 1].set_title('True Labels\\nGround Truth', fontsize=12, pad=15)\n",
    "        true_scatter.legend(title=\"Categories\", bbox_to_anchor=(1.02, 1), loc='upper left', title_fontsize=10)\n",
    "        \n",
    "        # Remove axis labels as they're not meaningful in t-SNE space\n",
    "        axes[idx, 0].set_xlabel('')\n",
    "        axes[idx, 0].set_ylabel('')\n",
    "        axes[idx, 1].set_xlabel('')\n",
    "        axes[idx, 1].set_ylabel('')\n",
    "        \n",
    "        # Add grid for better readability\n",
    "        axes[idx, 0].grid(True, alpha=0.3)\n",
    "        axes[idx, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/dataset/train_sel_hclust.csv')\n",
    "#df = pd.read_csv('../dataset/train_sel_hclust.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data, labels = preprocess_data_cluster(df)\n",
    "\n",
    "# Perform and evaluate K-Means\n",
    "kmeans_6_labels, kmeans_6 = kmeans_clustering(scaled_data, n_clusters=6)\n",
    "kmeans_6_scores = evaluate_clustering(scaled_data, kmeans_6_labels)\n",
    "save_cluster(kmeans_6, \"kmeans_6.pkl\")\n",
    "save_labels(kmeans_6_labels, \"kmeans_6_labels.pkl\")\n",
    "\n",
    "kmeans_5_labels, kmeans_5 = kmeans_clustering(scaled_data, n_clusters=5)\n",
    "kmeans_5_scores = evaluate_clustering(scaled_data, kmeans_5_labels)\n",
    "save_cluster(kmeans_5, \"kmeans_5.pkl\")\n",
    "save_labels(kmeans_5_labels, \"kmeans_5_labels.pkl\")\n",
    "\n",
    "# Perform and evaluate DBSCAN\n",
    "dbscan_labels = dbscan_clustering(scaled_data)\n",
    "dbscan_scores = evaluate_clustering(scaled_data, dbscan_labels)\n",
    "save_labels(dbscan_labels, \"dbscan_labels.pkl\")\n",
    "\n",
    "# Perform and evaluate Hierarchical Clustering\n",
    "\"\"\"hierarchical_labels = hierarchical_clustering(scaled_data, n_clusters=6)\n",
    "hierarchical_scores = evaluate_clustering(scaled_data, hierarchical_labels)\n",
    "save_labels(hierarchical_labels, \"hierarchical_labels.pkl\")\"\"\"\n",
    "\n",
    "# Display Evaluation Results in a Table\n",
    "evaluation_table = [\n",
    "    [\"K-Means-6\", *kmeans_6_scores],\n",
    "    [\"K-Means-5\", *kmeans_5_scores],\n",
    "    [\"DBSCAN\", *dbscan_scores]\n",
    "    #[\"Hierarchical\", *hierarchical_scores]\n",
    "]\n",
    "headers = [\"Method\", \"Silhouette Score\", \"Calinski-Harabasz Score\", \"Davies-Bouldin Score\"]\n",
    "print(\"\\nClustering Evaluation Summary:\")\n",
    "print(tabulate(evaluation_table, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "# Visualize Clustering Results Side by Side\n",
    "visualize_clusters(\n",
    "    scaled_data,\n",
    "    [kmeans_6_labels, kmeans_5_labels, dbscan_labels],\n",
    "    labels,\n",
    "    [\"K-Means/6\",\"K-Means/5\", f\"DBSCAN/{len(dbscan_labels)}\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10416.657438,
   "end_time": "2024-12-12T18:42:54.811900",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-12T15:49:18.154462",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
