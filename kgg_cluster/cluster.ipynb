{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19e1d1d",
   "metadata": {
    "papermill": {
     "duration": 0.002677,
     "end_time": "2024-12-12T15:49:20.914618",
     "exception": false,
     "start_time": "2024-12-12T15:49:20.911941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CLUSTER 6 CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea30827c",
   "metadata": {},
   "source": [
    "This notebook analyzes network traffic data using clustering techniques to identify patterns in network attacks. We'll explore two different approaches:\n",
    "1. Standard K-means clustering with 6 clusters\n",
    "2. Modified approach using DBSCAN with merged DoS/DDoS categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d827f2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T15:49:20.921122Z",
     "iopub.status.busy": "2024-12-12T15:49:20.920724Z",
     "iopub.status.idle": "2024-12-12T15:49:23.660503Z",
     "shell.execute_reply": "2024-12-12T15:49:23.659444Z"
    },
    "papermill": {
     "duration": 2.745925,
     "end_time": "2024-12-12T15:49:23.663002",
     "exception": false,
     "start_time": "2024-12-12T15:49:20.917077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from rich import print\n",
    "from tabulate import tabulate\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60ee1b0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T15:49:23.669390Z",
     "iopub.status.busy": "2024-12-12T15:49:23.668548Z",
     "iopub.status.idle": "2024-12-12T15:49:30.637782Z",
     "shell.execute_reply": "2024-12-12T15:49:30.636517Z"
    },
    "papermill": {
     "duration": 6.975208,
     "end_time": "2024-12-12T15:49:30.640504",
     "exception": false,
     "start_time": "2024-12-12T15:49:23.665296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/dataset/train_sel_hclust.csv')\n",
    "#df = pd.read_csv('../dataset/train_sel_hclust.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, scaler, label_encoder, base_filename):\n",
    "    \"\"\"\n",
    "    Save model and associated transformers.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained clustering model\n",
    "    - scaler: fitted scaler\n",
    "    - label_encoder: fitted label encoder\n",
    "    - base_filename: base name for the saved files\n",
    "    \"\"\"\n",
    "    joblib.dump(model, f'{base_filename}_model.joblib')\n",
    "    joblib.dump(scaler, f'{base_filename}_scaler.joblib')\n",
    "    joblib.dump(label_encoder, f'{base_filename}_encoder.joblib')\n",
    "\n",
    "def load_model(base_filename):\n",
    "    \"\"\"\n",
    "    Load saved model and associated transformers.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_filename: base name for the saved files\n",
    "    \n",
    "    Returns:\n",
    "    - model, scaler, label_encoder\n",
    "    \"\"\"\n",
    "    model = joblib.load(f'{base_filename}_model.joblib')\n",
    "    scaler = joblib.load(f'{base_filename}_scaler.joblib')\n",
    "    label_encoder = joblib.load(f'{base_filename}_encoder.joblib')\n",
    "    return model, scaler, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4fd9f7",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f86d6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df, method='median_multiplier', multiplier=2, column='category'):\n",
    "    \"\"\"\n",
    "    Balance the dataset by downsampling majority classes.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with 'category' or 'attack' column\n",
    "    - method: str, approach for calculating threshold\n",
    "    - multiplier: float, multiplier for threshold calculation\n",
    "    \n",
    "    Returns:\n",
    "    - balanced DataFrame\n",
    "    \"\"\"\n",
    "    counts = df[column].value_counts()\n",
    "\n",
    "    # Calculate threshold\n",
    "    if method == 'median_multiplier':\n",
    "        threshold = np.median(counts) * multiplier\n",
    "    elif method == 'mean_multiplier':\n",
    "        threshold = np.mean(counts) * multiplier\n",
    "    elif method == 'quantile':\n",
    "        threshold = counts.quantile(0.75)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'median_multiplier', 'mean_multiplier', or 'quantile'\")\n",
    "    \n",
    "    # Balance categories\n",
    "    balanced_dfs = []\n",
    "    for value in counts.index:\n",
    "        value_df = df[df[column] == value]\n",
    "        if len(value_df) > threshold:\n",
    "            value_df = resample(value_df, replace=False, n_samples=int(threshold), random_state=42)\n",
    "        balanced_dfs.append(value_df)\n",
    "    \n",
    "    balanced_df = pd.concat(balanced_dfs)\n",
    "    comparison_data = [\n",
    "        [value, counts[value], balanced_df[column].value_counts().get(value, 0)]\n",
    "        for value in sorted(counts.index)\n",
    "    ]\n",
    "\n",
    "    comparison_data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(tabulate(\n",
    "        comparison_data,\n",
    "        headers=[column, 'Original', 'After Balance'],\n",
    "        tablefmt='psql'\n",
    "    ))\n",
    "    \n",
    "    return balanced_df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"Prepare features for clustering.\"\"\"\n",
    "    return df.drop(['category', 'attack', 'is_benign'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5292e",
   "metadata": {},
   "source": [
    "## 3. Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074512c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clustering_metrics(X_scaled, clusters):\n",
    "    \"\"\"Calculate clustering quality metrics.\"\"\"\n",
    "    if len(set(clusters)) > 1:\n",
    "        metrics = {\n",
    "            'Silhouette Score': silhouette_score(X_scaled, clusters),\n",
    "            'Calinski-Harabasz Score': calinski_harabasz_score(X_scaled, clusters),\n",
    "            'Davies-Bouldin Score': davies_bouldin_score(X_scaled, clusters)\n",
    "        }\n",
    "        print(\"\\nClustering Quality Metrics:\")\n",
    "        print(tabulate(\n",
    "            metrics.items(),\n",
    "            headers=['Metric', 'Score'],\n",
    "            tablefmt='psql'\n",
    "        ))\n",
    "    \n",
    "def visualize_clusters(X_scaled, clusters, model=None, title_suffix=\"\"):\n",
    "    \"\"\"Visualize clusters using t-SNE.\"\"\"\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1],\n",
    "                         c=clusters,\n",
    "                         cmap='viridis',\n",
    "                         alpha=0.6)\n",
    "    \n",
    "    if hasattr(model, 'cluster_centers_'):\n",
    "        centers_tsne = tsne.fit_transform(model.cluster_centers_)\n",
    "        plt.scatter(centers_tsne[:, 0], centers_tsne[:, 1],\n",
    "                   c='red', marker='x', s=200, linewidths=3,\n",
    "                   label='Cluster Centers')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.xlabel('t-SNE dimension 1')\n",
    "    plt.ylabel('t-SNE dimension 2')\n",
    "    plt.title(f't-SNE Cluster Visualization {title_suffix}')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, scaler, base_filename):\n",
    "    \"\"\"Save model and associated transformers.\"\"\"\n",
    "    joblib.dump(model, f'{base_filename}_model.joblib')\n",
    "    joblib.dump(scaler, f'{base_filename}_scaler.joblib')\n",
    "\n",
    "def load_model(base_filename):\n",
    "    \"\"\"Load saved model and associated transformers.\"\"\"\n",
    "    model = joblib.load(f'{base_filename}_model.joblib')\n",
    "    scaler = joblib.load(f'{base_filename}_scaler.joblib')\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a75e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clustering_analysis(df, model_type='kmeans', label_column=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Run clustering analysis with specified model type and parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data\n",
    "    - model_type: str, one of ['kmeans', 'dbscan', 'attack_based']\n",
    "    - label_column: str, column name to use for labels (required for attack_based)\n",
    "    - **kwargs: additional parameters for specific clustering methods\n",
    "        For kmeans: n_clusters (default=6)\n",
    "        For dbscan: eps (default=0.3), min_samples (default=10)\n",
    "    \n",
    "    Returns:\n",
    "    - tuple of (model, scaler, clusters, label_encoder if applicable)\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Running {model_type.upper()} Analysis ===\")\n",
    "    \n",
    "    # Extract parameters from kwargs with defaults\n",
    "    to_merge = kwargs.get('to_merge', False)\n",
    "    scaler_type = kwargs.get('scaler_type', 'standard')\n",
    "    \n",
    "    if to_merge:\n",
    "        df['category'] = df['category'].replace({\n",
    "            'DDoS': 'DOS_DDOS',\n",
    "            'DoS': 'DOS_DDOS'\n",
    "        })\n",
    "    \n",
    "    df = balance_dataset(df)\n",
    "    scaler = StandardScaler() if scaler_type == 'standard' else MinMaxScaler()\n",
    "\n",
    "    if model_type == 'kmeans':\n",
    "        n_clusters = df[label_column].nunique()\n",
    "        X = prepare_features(df)\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        clusters = model.fit_predict(X_scaled)\n",
    "        \n",
    "        calculate_clustering_metrics(X_scaled, clusters)\n",
    "        visualize_clusters(X_scaled, clusters, model, \"K-Means\")\n",
    "        print(f\"\\nClustering time: {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        return model, scaler, clusters\n",
    "    \n",
    "    elif model_type == 'dbscan':\n",
    "        X = prepare_features(df)\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        eps = kwargs.get('eps', 0.3)\n",
    "        min_samples = kwargs.get('min_samples', 10)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        clusters = model.fit_predict(X_scaled)\n",
    "        \n",
    "        calculate_clustering_metrics(X_scaled, clusters)\n",
    "        visualize_clusters(X_scaled, clusters, title_suffix=\"DBSCAN\")\n",
    "        print(f\"\\nClustering time: {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        return model, scaler, clusters\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"model_type must be one of ['kmeans', 'dbscan']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aef324",
   "metadata": {},
   "source": [
    "## 4. Analysis: K-means Clustering\n",
    "\n",
    "### Why K-means?\n",
    "\n",
    "We start with K-means clustering because:\n",
    "1. It's a simple and effective algorithm for our initial analysis\n",
    "2. We have a reasonable assumption about the number of clusters (based on attack categories)\n",
    "3. It works well with standardized numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1dbcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, scaler, clusters = run_clustering_analysis(df, model_type='kmeans', label_column='category')\n",
    "save_model(model, scaler, 'kmeans_model_category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2d3b4",
   "metadata": {},
   "source": [
    "## 5. Modified Approach: DBSCAN with Merged Categories\n",
    "\n",
    "### Why modify the approach?\n",
    "\n",
    "Based on the K-means results, we observed:\n",
    "1. Significant overlap between DDoS and DoS categories\n",
    "2. Some clusters might not be spherical (a limitation of K-means)\n",
    "3. StandardScaler might not be optimal for our feature distributions\n",
    "\n",
    "Therefore, we implement these changes:\n",
    "1. Merge DDoS and DoS into a single category\n",
    "2. Use DBSCAN for non-spherical clusters\n",
    "3. Switch to MinMaxScaler for better feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74635a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, scaler, clusters = run_clustering_analysis(df, model_type='dbscan', label_column='category', to_merge=True)\n",
    "save_model(model, scaler, 'dbscan_model_category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f34649",
   "metadata": {},
   "source": [
    "Given the results, it is plausible to assert that without any external assistance, I have successfully identified 17 categories. It is not unreasonable to hypothesize that I could potentially identify all 19 categories in the context of network attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, scaler, clusters = run_clustering_analysis(df, model_type='kmeans', label_column='attack')\n",
    "save_model(model, scaler, 'kmeans_model_attack')\n",
    "\n",
    "model, scaler, clusters = run_clustering_analysis(df, model_type='dbscan', label_column='attack')\n",
    "save_model(model, scaler, 'dbscan_model_attack')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10416.657438,
   "end_time": "2024-12-12T18:42:54.811900",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-12T15:49:18.154462",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
